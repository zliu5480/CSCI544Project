{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5090f8f2-74e6-4176-b29b-da14173937df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as datasets\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e0be61-d2fb-4cf1-a2c9-8c13f2675bdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/glove.twitter.27B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-12e077e500e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mglove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mGloVe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-12e077e500e1>\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./models/glove.twitter.27B.50d.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mglove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/glove.twitter.27B.50d.txt'"
     ]
    }
   ],
   "source": [
    "def load_model(file_path = \"./models/glove.twitter.27B.50d.txt\"):\n",
    "    glove = {}\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            items = lines.split()\n",
    "            if len(items) != 51:\n",
    "                continue\n",
    "            else:\n",
    "                word_vector = []\n",
    "                for i in range(1,51):\n",
    "                    word_vector.append(float(items[i]))\n",
    "                glove[items[0]] = word_vector\n",
    "    return glove\n",
    "GloVe = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a528b49-e967-4262-9770-c2aeb61277ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"i'm\" : \"i am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"i'd\" : \"i would\",\n",
    "    \"i've\" : \"i have\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"that'll\" : \"that will\",\n",
    "    \"that'd\" : \"that would\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what'd\" : \"what would\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"where'll\" : \"where will\",\n",
    "    \"where'd\" : \"where would\",\n",
    "    \"when's\" : \"when is\",\n",
    "    \"when'll\" : \"when will\",\n",
    "    \"when'd\" : \"when would\",\n",
    "    \"why's\" : \"why is\",\n",
    "    \"why'll\" : \"why will\",\n",
    "    \"why'd\" : \"why would\",\n",
    "    \"how's\" : \"how is\",\n",
    "    \"how'll\" : \"how will\",\n",
    "    \"how'd\" : \"how would\",\n",
    "    \"would've\" : \"would have\",\n",
    "    \"should've\" : \"should have\",\n",
    "    \"could've\" : \"could have\",\n",
    "    \"might've\" : \"might have\",\n",
    "    \"must've\" : \"must have\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"wasn't\" : \"was not\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\"\n",
    "}\n",
    "\n",
    "def contractionfunction(s):\n",
    "    if s in contractions_dict.keys():\n",
    "        return contractions_dict[s]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd363bd-7bc9-43c9-90c4-8a90a92389bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_replace = {\",\",\".\",\"/\",\";\",\"'\",\"[\",\"]\",\"\\\\\",\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"-\",\"_\",\"=\",\"+\",\"<\",\">\",\"?\",\":\",\"\\\"\",\"{\",\"}\",\"|\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a59181d-d719-45a4-adb0-0a2f98b74130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(file_path = \"./data/emotion_data_train.txt\"):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            y_x = lines.split(\"|\")\n",
    "            if len(y_x) < 2:\n",
    "                continue\n",
    "            train_y.append(y_x[0])\n",
    "            train_x.append(y_x[1])\n",
    "    return train_x, train_y\n",
    "\n",
    "def load_dev(file_path = \"./data/emotion_data_test.txt\"):\n",
    "    dev_x = []\n",
    "    dev_y = []\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            y_x = lines.split(\"|\")\n",
    "            if len(y_x) < 2:\n",
    "                continue\n",
    "            dev_y.append(y_x[0])\n",
    "            dev_x.append(y_x[1])\n",
    "    return dev_x, dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "167a860b-3165-4f6e-aa90-826094623d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_raw, train_y_raw = load_train()\n",
    "dev_x_raw, dev_y_raw = load_dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "33697347-a7e1-425f-bd8e-d21267e094b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7016\n",
      "7016\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x_raw))\n",
    "print(len(train_y_raw))\n",
    "print(len(dev_x_raw))\n",
    "print(len(dev_y_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16e258fc-16e0-4d14-b76d-242a48b758a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#Data preprocess:\n",
    "def preprocess(sentence_list):\n",
    "    for i in range(len(sentence_list)):\n",
    "        sentence_list[i] = sentence_list[i].lower()\n",
    "        for char in char_replace:\n",
    "            if char in sentence_list[i]:\n",
    "                sentence_list[i] = sentence_list[i].replace(char, \" \")\n",
    "        sentence_list[i] = ' '.join([word for word in sentence_list[i].split() if word not in (stop)])\n",
    "        sentence_list[i] = sentence_list[i].split()\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b22fee5e-68e1-4c3d-a56b-0f95d28a8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_raw = preprocess(train_x_raw)\n",
    "dev_x_raw = preprocess(dev_x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "03f8da6c-b4eb-4f4c-a89a-37ce936e4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_count = {}\n",
    "for y in train_y_raw:\n",
    "    if y in y_count:\n",
    "        y_count[y] += 1\n",
    "    else:\n",
    "        y_count[y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2334530c-80ad-4441-b00b-6c03aaa18667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joy': 1020,\n",
       " 'fear': 1002,\n",
       " 'anger': 1005,\n",
       " 'sadness': 1009,\n",
       " 'disgust': 995,\n",
       " 'shame': 1000,\n",
       " 'guilt': 985}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5dbca5a3-7efe-431d-aa1c-e22848ea8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(a):\n",
    "    return a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78c375f3-e9e4-49cf-9815-f27870da2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for key in y_count:\n",
    "    item = y_count[key]\n",
    "    label_list.append((key,item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7e6087be-8ca2-412a-8821-5be051392115",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list.sort(reverse=True,key=sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "23619a36-b03f-49ba-a0f3-f7e75e687045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('joy', 1020),\n",
       " ('sadness', 1009),\n",
       " ('anger', 1005),\n",
       " ('fear', 1002),\n",
       " ('shame', 1000),\n",
       " ('disgust', 995),\n",
       " ('guilt', 985)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "77179920-4d54-4ffe-9e88-08060cb91c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {}\n",
    "i = 0\n",
    "for label in label_list:\n",
    "    label2id[label[0]] = i\n",
    "    label2id[i] = label[0]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "54593af0-6ef1-46ad-b374-1d79811bb085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joy': 0,\n",
       " 0: 'joy',\n",
       " 'sadness': 1,\n",
       " 1: 'sadness',\n",
       " 'anger': 2,\n",
       " 2: 'anger',\n",
       " 'fear': 3,\n",
       " 3: 'fear',\n",
       " 'shame': 4,\n",
       " 4: 'shame',\n",
       " 'disgust': 5,\n",
       " 5: 'disgust',\n",
       " 'guilt': 6,\n",
       " 6: 'guilt'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "755ee3f8-4856-4208-b40c-a5a56f0c500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_glove(unk):\n",
    "    digits = 0\n",
    "    for c in unk:\n",
    "        if c.isdigit():\n",
    "            digits += 1\n",
    "    df = digits/len(unk)\n",
    "    if unk.isdigit():\n",
    "        return 1.0\n",
    "    elif df > 0.5:\n",
    "        return 2.0\n",
    "    elif digits>0:\n",
    "        return 3.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6eaad76b-0709-4854-b5c7-5154e2450c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"< UNK >\"\n",
    "GloVe[UNK] = np.random.uniform(-0.25, 0.25, 200).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8c0a2c98-547b-4997-88c8-ec8c0e0c8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 0\n",
    "for sentence in train_x_raw:\n",
    "    if len(sentence) > max_sentence_length:\n",
    "        max_sentence_length = len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ae428424-0f6c-42eb-88b9-0688aa22760d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c9d04046-2563-4078-b288-e95a9effeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class average_meter(object):\n",
    "    '''Computes and stores the average and current value\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "00832f0e-8f80-4251-b2c4-b87bdfdd96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotion_dataset(datasets.Dataset):\n",
    "    \n",
    "    def __init__(self, word_lists, label_lists):\n",
    "        self.word_lists = word_lists\n",
    "        self.label_lists = label_lists\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.word_lists[index], self.label_lists[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5cffe8ac-ba00-443a-bfe3-d1c31afa2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data):\n",
    "    b_size = len(data)\n",
    "    x_tensor = torch.zeros(b_size, 30, 201)\n",
    "    y_tensor = torch.zeros(b_size).long()\n",
    "    \n",
    "    for b_index in range(b_size):\n",
    "        x = data[b_index][0]\n",
    "        y = data[b_index][1]\n",
    "        for xy_index in range(min(len(x),30)):\n",
    "            word = x[xy_index]\n",
    "            if word in GloVe:\n",
    "                x_vector = deepcopy(GloVe[word])\n",
    "                one_more = get_type_glove(word)\n",
    "                x_vector.append(one_more)\n",
    "                _x = torch.FloatTensor(x_vector)\n",
    "                x_tensor[b_index][xy_index] = _x\n",
    "            else:\n",
    "                unk_vector = deepcopy(GloVe[UNK])\n",
    "                one_more = get_type_glove(word)\n",
    "                unk_vector.append(one_more)\n",
    "                _unk = torch.FloatTensor(unk_vector)\n",
    "                x_tensor[b_index][xy_index] = _unk\n",
    "        y_tensor[b_index] = label2id[y]\n",
    "    data_len = []\n",
    "    for xy in data:\n",
    "        data_len.append(len(xy[0]))\n",
    "    return x_tensor, y_tensor, data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5d12ff82-0b63-448a-aebb-f20119cc1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 512\n",
    "DEV_BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5859dd6c-ca00-48f7-9add-f70b26775541",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = emotion_dataset(train_x_raw,train_y_raw)\n",
    "dev_dataset = emotion_dataset(dev_x_raw,dev_y_raw)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, collate_fn=data_process)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=DEV_BATCH_SIZE, shuffle=False, collate_fn=data_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b60d8b3f-87fe-4656-9090-d333eb2f11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 36\n",
    "        n_classes = 7\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, 201)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        logit = self.fc1(x) \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "664c2642-5cca-4350-90a5-eb1bd9c1f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "n_epochs = 50\n",
    "model = CNN_Text().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a27abaa1-83de-4fb1-a750-38b052b3f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = average_meter()\n",
    "    model.train()\n",
    "    total = len(train_loader)\n",
    "    for i in train_loader:\n",
    "        x = i[0].to(DEVICE)\n",
    "        y = i[1].to(DEVICE)\n",
    "        l = i[2]\n",
    "        pred = model.forward(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(pred, y).to(DEVICE)\n",
    "        train_loss.update(loss.item(),x.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss.avg\n",
    "        \n",
    "def validate(model, dev_loader, criterion, optimizer, epoch):\n",
    "    b_size = DEV_BATCH_SIZE\n",
    "    valid_loss = average_meter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "        for i in dev_loader:\n",
    "            x = i[0].to(DEVICE)\n",
    "            y = i[1].to(DEVICE)\n",
    "            l = i[2]\n",
    "            pred = model.forward(x)\n",
    "            loss = criterion(pred, y).to(DEVICE)\n",
    "            valid_loss.update(loss.item(),x.size(0))\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            correct += (pred == y).float().sum()\n",
    "            total += y.shape[0]\n",
    "            step += 1\n",
    "    return valid_loss.avg, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4190b2cb-a622-420f-bc63-7d09ed35f3a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-231-d53da4480a78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-230-fee1b7f3ca57>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APPs\\Annaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APPs\\Annaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APPs\\Annaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-196-36c61e4839c9>\u001b[0m in \u001b[0;36mdata_process\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mone_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_type_glove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mx_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_more\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[0mx_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxy_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "acc_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    valid_loss, valid_acc = validate(model, dev_dataloader, criterion, optimizer, epoch)\n",
    "    if valid_acc > best_acc:\n",
    "        acc_model = deepcopy(model)\n",
    "        best_acc = valid_acc\n",
    "    print(\"{}%, Epoch {}, train loss {:.8f}, validate loss {:.8f}, acc {:.8f}\".format(100*(epoch+1)/n_epochs,str(epoch).zfill(2),train_loss,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a3e3ad23-ce8d-488d-a5f6-efe76a33fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(s):\n",
    "    s = s.lower()\n",
    "    for char in char_replace:\n",
    "        if char in s:\n",
    "            s = s.replace(char, \" \")\n",
    "    s = ' '.join([word for word in s.split() if word not in (stop)])\n",
    "    s = s.split()\n",
    "    s_tensor = torch.zeros(1, 80, 201)\n",
    "    for i in range(len(s)):\n",
    "        word = s[i]\n",
    "        if word in GloVe:\n",
    "            x_vector = deepcopy(GloVe[word])\n",
    "            one_more = get_type_glove(word)\n",
    "            x_vector.append(one_more)\n",
    "            _x = torch.FloatTensor(x_vector)\n",
    "            s_tensor[0][i] = _x\n",
    "        else:\n",
    "            unk_vector = deepcopy(GloVe[UNK])\n",
    "            one_more = get_type_glove(word)\n",
    "            unk_vector.append(one_more)\n",
    "            _unk = torch.FloatTensor(unk_vector)\n",
    "            s_tensor[0][i] = _unk\n",
    "    return s_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "edb0397a-ca17-4461-9d0b-6ba156476695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.4147,  0.2790, -2.4521, -4.2422, -2.1910, -3.6526, -0.8217]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I feel very happy\"\n",
    "s = sentence_to_vec(sentence).to(DEVICE)\n",
    "pred_s = model.forward(s)\n",
    "pred_y = torch.max(pred_s, 1)[1]\n",
    "print(pred_s)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "34fba765-1f20-497d-b9ad-3f14d7ceefc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joy': 0,\n",
       " 0: 'joy',\n",
       " 'sadness': 1,\n",
       " 1: 'sadness',\n",
       " 'anger': 2,\n",
       " 2: 'anger',\n",
       " 'fear': 3,\n",
       " 3: 'fear',\n",
       " 'shame': 4,\n",
       " 4: 'shame',\n",
       " 'disgust': 5,\n",
       " 5: 'disgust',\n",
       " 'guilt': 6,\n",
       " 6: 'guilt'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "83f6f25c-20af-4752-8079-6cb6049afcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=torch.load(\"emotional_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "34f471c1-5aa0-4f0a-8efc-9a465c4dc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predict_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=len(dev_dataset), shuffle=False, collate_fn=data_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4f494f03-3467-4162-b8ab-f837626530d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "    step = 0\n",
    "    for i in dev_predict_loader:\n",
    "        x = i[0].to(DEVICE)\n",
    "        y = i[1].to(DEVICE)\n",
    "        l = i[2]\n",
    "        pred = model_1.forward(x)\n",
    "        pred = torch.max(pred, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        step += 1\n",
    "acc = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "71342e54-5acb-4e55-a562-4edc38519753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9920, device='cuda:0')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "91db22b5-e8ab-4868-9b09-c15c911d98d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 0, 3, 2, 1, 5,\n",
       "        4, 6, 0, 3, 2, 3, 2, 1, 5, 4, 6, 0, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0,\n",
       "        3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2,\n",
       "        0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 5, 4, 6, 0, 3, 2, 5,\n",
       "        4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0,\n",
       "        3, 2, 1, 5, 4, 6, 0, 3, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5,\n",
       "        4, 6, 0, 3, 2, 1, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4,\n",
       "        6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 1, 5, 4, 6, 0,\n",
       "        3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1,\n",
       "        5, 4, 6, 0, 3, 2, 1, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0,\n",
       "        3, 2, 1, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 4, 6,\n",
       "        0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 3, 2, 1, 5,\n",
       "        1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6,\n",
       "        0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 0, 3, 2, 1, 5,\n",
       "        4, 6, 0, 3, 3, 2, 1, 5, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3,\n",
       "        2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 2, 1, 0, 3, 2, 1, 5, 4, 6, 0, 3,\n",
       "        2, 1, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 5, 4, 0, 3, 2, 5, 4, 6, 0, 1,\n",
       "        0, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 3,\n",
       "        2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 5, 4, 4, 6, 0, 3, 2, 1, 5, 4, 3, 2, 1, 5,\n",
       "        4, 6, 0, 3, 2, 1, 4, 3, 2, 1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 6, 0, 3, 3, 2,\n",
       "        1, 5, 4, 6, 0, 3, 2, 1, 5, 4, 0, 3, 2, 1, 5, 4, 6, 0, 3, 2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "16c36bbe-18e1-407d-b1dc-adbf4b2d85fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 1, 5, 4, 6, 0, 4, 5, 3, 4, 4, 5, 2, 3, 5, 4, 5, 5, 0, 3, 2, 1, 5,\n",
       "        3, 4, 2, 3, 6, 0, 1, 5, 5, 5, 2, 0, 1, 5, 4, 5, 0, 5, 2, 1, 2, 6, 3, 3,\n",
       "        3, 2, 2, 2, 4, 2, 0, 3, 2, 6, 5, 3, 4, 6, 0, 6, 0, 4, 5, 4, 6, 0, 3, 1,\n",
       "        0, 3, 3, 0, 3, 3, 4, 6, 3, 1, 1, 5, 0, 2, 2, 5, 4, 5, 3, 6, 0, 3, 6, 5,\n",
       "        4, 1, 3, 3, 2, 1, 5, 4, 1, 0, 3, 6, 1, 5, 4, 6, 0, 3, 5, 3, 3, 0, 2, 0,\n",
       "        3, 2, 1, 5, 3, 6, 0, 3, 3, 3, 4, 2, 0, 3, 2, 1, 4, 1, 6, 0, 3, 2, 1, 3,\n",
       "        0, 2, 0, 3, 6, 1, 2, 5, 0, 5, 5, 0, 3, 2, 1, 2, 6, 4, 0, 3, 2, 6, 5, 2,\n",
       "        6, 1, 1, 4, 1, 1, 6, 2, 3, 3, 2, 1, 5, 5, 3, 0, 3, 2, 1, 1, 0, 6, 2, 2,\n",
       "        2, 4, 1, 5, 4, 2, 0, 3, 2, 1, 5, 3, 6, 0, 2, 2, 1, 4, 2, 3, 0, 3, 2, 1,\n",
       "        1, 4, 6, 0, 5, 5, 1, 4, 4, 0, 3, 2, 0, 4, 4, 4, 0, 3, 0, 1, 2, 4, 6, 0,\n",
       "        3, 2, 1, 0, 2, 4, 4, 0, 6, 2, 1, 0, 4, 6, 0, 3, 4, 1, 2, 4, 2, 0, 0, 2,\n",
       "        0, 3, 5, 1, 2, 4, 5, 0, 3, 0, 0, 4, 3, 6, 0, 3, 5, 1, 5, 4, 3, 2, 5, 4,\n",
       "        3, 6, 4, 6, 0, 3, 2, 1, 2, 4, 4, 4, 2, 1, 5, 4, 6, 0, 3, 2, 1, 6, 0, 0,\n",
       "        0, 3, 2, 6, 5, 4, 1, 0, 3, 2, 5, 2, 5, 4, 0, 3, 2, 1, 5, 0, 3, 0, 2, 6,\n",
       "        2, 0, 0, 3, 3, 2, 1, 5, 0, 3, 2, 1, 2, 2, 6, 0, 4, 2, 1, 2, 4, 6, 0, 0,\n",
       "        6, 0, 0, 5, 3, 1, 3, 2, 1, 5, 4, 3, 0, 2, 4, 0, 4, 2, 0, 6, 2, 2, 0, 1,\n",
       "        1, 1, 2, 4, 0, 3, 2, 1, 5, 4, 6, 4, 3, 2, 5, 4, 5, 3, 6, 5, 6, 6, 4, 4,\n",
       "        0, 2, 0, 3, 2, 1, 5, 4, 6, 0, 1, 2, 1, 3, 4, 1, 0, 0, 2, 1, 5, 4, 6, 3,\n",
       "        6, 1, 0, 0, 1, 0, 6, 2, 6, 2, 5, 6, 1, 6, 5, 3, 2, 1, 5, 5, 3, 2, 1, 5,\n",
       "        4, 6, 0, 3, 2, 1, 2, 3, 2, 1, 5, 4, 5, 0, 3, 4, 1, 6, 4, 4, 0, 0, 3, 2,\n",
       "        1, 5, 4, 6, 0, 3, 3, 3, 6, 4, 0, 2, 1, 2, 5, 4, 6, 0, 3, 6],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5c9064c6-b3a4-4d1a-a044-4175ab967cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emotion_GloVe(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Emotion_GloVe, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size = 201,\n",
    "                                 hidden_size = 256,\n",
    "                                 num_layers = 1,\n",
    "                                 bidirectional=True)\n",
    "        self.fc1 = nn.Linear(2* 256, 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, 30, 256).requires_grad_().to(DEVICE)\n",
    "        c0 = torch.zeros(2, 30, 256).requires_grad_().to(DEVICE)\n",
    "        output, h = self.lstm(x, (h0, c0))\n",
    "        output = output[:, -1, :]\n",
    "        output = F.dropout(output, 0.5)\n",
    "        output = self.fc1(output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "299e74e2-4ff7-4ae0-a72b-7f8d7cf7d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "model = Emotion_GloVe().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "03e17958-78a7-4c5b-9d97-b9f0e4f56c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0%, Epoch 00, train loss 2.01010158, validate loss 2.01973648, acc 0.14600001\n",
      "10.0%, Epoch 01, train loss 2.02004334, validate loss 2.01705277, acc 0.14800000\n",
      "15.0%, Epoch 02, train loss 2.02254717, validate loss 2.01742150, acc 0.14800000\n",
      "20.0%, Epoch 03, train loss 2.02259445, validate loss 2.01728049, acc 0.14800000\n",
      "25.0%, Epoch 04, train loss 2.02259930, validate loss 2.01742007, acc 0.14800000\n",
      "30.0%, Epoch 05, train loss 2.02259353, validate loss 2.01742150, acc 0.14800000\n",
      "35.0%, Epoch 06, train loss 2.02260491, validate loss 2.01742139, acc 0.14800000\n",
      "40.0%, Epoch 07, train loss 2.02260478, validate loss 2.01742149, acc 0.14800000\n",
      "45.0%, Epoch 08, train loss 2.02259975, validate loss 2.01742152, acc 0.14800000\n",
      "50.0%, Epoch 09, train loss 2.02260226, validate loss 2.01742149, acc 0.14800000\n",
      "55.0%, Epoch 10, train loss 2.02260026, validate loss 2.01742355, acc 0.14800000\n",
      "60.0%, Epoch 11, train loss 2.02259781, validate loss 2.01742149, acc 0.14800000\n",
      "65.0%, Epoch 12, train loss 2.02259930, validate loss 2.01742146, acc 0.14800000\n",
      "70.0%, Epoch 13, train loss 2.02260146, validate loss 2.01742091, acc 0.14800000\n",
      "75.0%, Epoch 14, train loss 2.02258586, validate loss 2.01741993, acc 0.14800000\n",
      "80.0%, Epoch 15, train loss 2.02253094, validate loss 2.01742150, acc 0.14800000\n",
      "85.0%, Epoch 16, train loss 2.02260510, validate loss 2.01742150, acc 0.14800000\n",
      "90.0%, Epoch 17, train loss 2.02260324, validate loss 2.01742147, acc 0.14800000\n",
      "95.0%, Epoch 18, train loss 2.02260409, validate loss 2.01938898, acc 0.14600001\n",
      "100.0%, Epoch 19, train loss 2.02260461, validate loss 2.01742150, acc 0.14800000\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "acc_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    valid_loss, valid_acc = validate(model, dev_dataloader, criterion, optimizer, epoch)\n",
    "    if valid_acc > best_acc:\n",
    "        acc_model = deepcopy(model)\n",
    "        best_acc = valid_acc\n",
    "    print(\"{}%, Epoch {}, train loss {:.8f}, validate loss {:.8f}, acc {:.8f}\".format(100*(epoch+1)/n_epochs,str(epoch).zfill(2),train_loss,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6cbb1541-d584-4b6d-af47-873a4984451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.h_size = 256\n",
    "        self.rnn = nn.GRU(201, 256, batch_first = True)\n",
    "        self.fc = nn.Linear(256, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.h_size).to(DEVICE)\n",
    "        out, hidden = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "81954871-2dd2-4323-abe8-c5d75645769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "model = GRUNet().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e5a8f38a-65f7-4e2e-a7ea-002be7f13748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = average_meter()\n",
    "    model.train()\n",
    "    total = len(train_loader)\n",
    "    for i in train_loader:\n",
    "        x = i[0].to(DEVICE)\n",
    "        y = i[1].to(DEVICE)\n",
    "        l = i[2]\n",
    "        pred, hidden = model.forward(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(pred, y).to(DEVICE)\n",
    "        train_loss.update(loss.item(),x.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss.avg\n",
    "        \n",
    "def validate(model, dev_loader, criterion, optimizer, epoch):\n",
    "    b_size = DEV_BATCH_SIZE\n",
    "    valid_loss = average_meter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "        for i in dev_loader:\n",
    "            x = i[0].to(DEVICE)\n",
    "            y = i[1].to(DEVICE)\n",
    "            l = i[2]\n",
    "            pred, hidden = model.forward(x)\n",
    "            loss = criterion(pred, y).to(DEVICE)\n",
    "            valid_loss.update(loss.item(),x.size(0))\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            correct += (pred == y).float().sum()\n",
    "            total += y.shape[0]\n",
    "            step += 1\n",
    "    return valid_loss.avg, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ea218e3f-238e-4eb4-921e-3e07dcd5b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0%, Epoch 00, train loss 16.04284865, validate loss 24.87347794, acc 0.14800000\n",
      "4.0%, Epoch 01, train loss 15.30215074, validate loss 9.10035419, acc 0.17000000\n",
      "6.0%, Epoch 02, train loss 8.12217488, validate loss 6.56886101, acc 0.20000000\n",
      "8.0%, Epoch 03, train loss 5.29472093, validate loss 4.67325211, acc 0.18600000\n",
      "10.0%, Epoch 04, train loss 4.00887791, validate loss 3.56146979, acc 0.22400001\n",
      "12.0%, Epoch 05, train loss 3.29977558, validate loss 3.17567205, acc 0.21800001\n",
      "14.0%, Epoch 06, train loss 2.78575575, validate loss 2.64670539, acc 0.24400002\n",
      "16.0%, Epoch 07, train loss 2.57165755, validate loss 2.47290254, acc 0.25400001\n",
      "18.0%, Epoch 08, train loss 2.45154045, validate loss 2.29291654, acc 0.25800002\n",
      "20.0%, Epoch 09, train loss 2.35066129, validate loss 2.43983483, acc 0.23000000\n",
      "22.0%, Epoch 10, train loss 2.30753548, validate loss 2.18877149, acc 0.28400001\n",
      "24.0%, Epoch 11, train loss 2.18666603, validate loss 2.22113991, acc 0.27400002\n",
      "26.0%, Epoch 12, train loss 2.08908469, validate loss 2.12086391, acc 0.28200001\n",
      "28.0%, Epoch 13, train loss 2.03608469, validate loss 2.08127761, acc 0.28200001\n",
      "30.0%, Epoch 14, train loss 1.96643460, validate loss 2.03126884, acc 0.28200001\n",
      "32.0%, Epoch 15, train loss 1.93462832, validate loss 2.00344515, acc 0.28400001\n",
      "34.0%, Epoch 16, train loss 1.92812355, validate loss 2.00250316, acc 0.28600001\n",
      "36.0%, Epoch 17, train loss 1.91750858, validate loss 2.00344419, acc 0.28600001\n",
      "38.0%, Epoch 18, train loss 1.90131169, validate loss 2.00082779, acc 0.28800002\n",
      "40.0%, Epoch 19, train loss 1.89020968, validate loss 1.98795676, acc 0.28600001\n",
      "42.0%, Epoch 20, train loss 1.88077324, validate loss 1.98587966, acc 0.28400001\n",
      "44.0%, Epoch 21, train loss 1.87370356, validate loss 1.98657608, acc 0.27800003\n",
      "46.0%, Epoch 22, train loss 1.86940439, validate loss 1.98128545, acc 0.27400002\n",
      "48.0%, Epoch 23, train loss 1.86462129, validate loss 1.97002876, acc 0.27800003\n",
      "50.0%, Epoch 24, train loss 1.85564890, validate loss 1.96669555, acc 0.28600001\n",
      "52.0%, Epoch 25, train loss 1.84715044, validate loss 1.96070349, acc 0.27600002\n",
      "54.0%, Epoch 26, train loss 1.84254899, validate loss 1.95318079, acc 0.28200001\n",
      "56.0%, Epoch 27, train loss 1.83869371, validate loss 1.94821906, acc 0.27400002\n",
      "58.0%, Epoch 28, train loss 1.83391573, validate loss 1.94663036, acc 0.27600002\n",
      "60.0%, Epoch 29, train loss 1.83023795, validate loss 1.94431376, acc 0.28400001\n",
      "62.0%, Epoch 30, train loss 1.82772144, validate loss 1.94276607, acc 0.28600001\n",
      "64.0%, Epoch 31, train loss 1.82501824, validate loss 1.94479644, acc 0.28800002\n",
      "66.0%, Epoch 32, train loss 1.82403775, validate loss 1.94331098, acc 0.29200003\n",
      "68.0%, Epoch 33, train loss 1.82290712, validate loss 1.94164371, acc 0.29600000\n",
      "70.0%, Epoch 34, train loss 1.82116421, validate loss 1.94011617, acc 0.29000002\n",
      "72.0%, Epoch 35, train loss 1.82010968, validate loss 1.93912172, acc 0.28800002\n",
      "74.0%, Epoch 36, train loss 1.81988752, validate loss 1.93931592, acc 0.29000002\n",
      "76.0%, Epoch 37, train loss 1.81986543, validate loss 1.93884122, acc 0.28600001\n",
      "78.0%, Epoch 38, train loss 1.82066022, validate loss 1.93711317, acc 0.29400000\n",
      "80.0%, Epoch 39, train loss 1.82079544, validate loss 1.93495417, acc 0.29800001\n",
      "82.0%, Epoch 40, train loss 1.82005224, validate loss 1.93367577, acc 0.30000001\n",
      "84.0%, Epoch 41, train loss 1.81795802, validate loss 1.93377209, acc 0.30400002\n",
      "86.0%, Epoch 42, train loss 1.81522052, validate loss 1.93394887, acc 0.30000001\n",
      "88.0%, Epoch 43, train loss 1.81291974, validate loss 1.93338192, acc 0.29400000\n",
      "90.0%, Epoch 44, train loss 1.81160243, validate loss 1.93280113, acc 0.29200003\n",
      "92.0%, Epoch 45, train loss 1.81067077, validate loss 1.93274832, acc 0.29400000\n",
      "94.0%, Epoch 46, train loss 1.80987061, validate loss 1.93277037, acc 0.29200003\n",
      "96.0%, Epoch 47, train loss 1.80916177, validate loss 1.93247664, acc 0.29200003\n",
      "98.0%, Epoch 48, train loss 1.80856960, validate loss 1.93233728, acc 0.29200003\n",
      "100.0%, Epoch 49, train loss 1.80812020, validate loss 1.93231666, acc 0.29200003\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "acc_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    valid_loss, valid_acc = validate(model, dev_dataloader, criterion, optimizer, epoch)\n",
    "    if valid_acc > best_acc:\n",
    "        acc_model = deepcopy(model)\n",
    "        best_acc = valid_acc\n",
    "    print(\"{}%, Epoch {}, train loss {:.8f}, validate loss {:.8f}, acc {:.8f}\".format(100*(epoch+1)/n_epochs,str(epoch).zfill(2),train_loss,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4505ff-58f5-48e9-a6a3-c9b7b173f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        filter_sizes = [2,3,4]\n",
    "        num_filters = 100\n",
    "        n_classes = 7\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, 201)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 7)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4deb1691-96fd-448d-b232-3b6fb1a5fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "n_epochs = 50\n",
    "model = CNN_Text().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c273f186-47ef-447d-b7b7-6abdf2e3fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\APPs\\Annaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0%, Epoch 00, train loss 1.94474753, validate loss 1.93987871, acc 0.14400001\n",
      "4.0%, Epoch 01, train loss 1.92040681, validate loss 1.88389413, acc 0.23800001\n",
      "6.0%, Epoch 02, train loss 1.86053390, validate loss 1.83678313, acc 0.30400002\n",
      "8.0%, Epoch 03, train loss 1.81620807, validate loss 1.80308235, acc 0.36800003\n",
      "10.0%, Epoch 04, train loss 1.78283451, validate loss 1.77127900, acc 0.41200003\n",
      "12.0%, Epoch 05, train loss 1.75160653, validate loss 1.75186491, acc 0.41000003\n",
      "14.0%, Epoch 06, train loss 1.72311918, validate loss 1.73176570, acc 0.42600003\n",
      "16.0%, Epoch 07, train loss 1.70071030, validate loss 1.71845526, acc 0.44200003\n",
      "18.0%, Epoch 08, train loss 1.68143491, validate loss 1.70948460, acc 0.44600001\n",
      "20.0%, Epoch 09, train loss 1.66665910, validate loss 1.70266666, acc 0.44600001\n",
      "22.0%, Epoch 10, train loss 1.65328116, validate loss 1.69610053, acc 0.45600003\n",
      "24.0%, Epoch 11, train loss 1.64310538, validate loss 1.69214867, acc 0.47200003\n",
      "26.0%, Epoch 12, train loss 1.63453327, validate loss 1.68754267, acc 0.48000002\n",
      "28.0%, Epoch 13, train loss 1.62526797, validate loss 1.68360655, acc 0.48400003\n",
      "30.0%, Epoch 14, train loss 1.61884025, validate loss 1.68031832, acc 0.48400003\n",
      "32.0%, Epoch 15, train loss 1.61244823, validate loss 1.67724347, acc 0.49400002\n",
      "34.0%, Epoch 16, train loss 1.60720900, validate loss 1.67514243, acc 0.49000001\n",
      "36.0%, Epoch 17, train loss 1.60136824, validate loss 1.67284056, acc 0.50200003\n",
      "38.0%, Epoch 18, train loss 1.59772536, validate loss 1.67228616, acc 0.49400002\n",
      "40.0%, Epoch 19, train loss 1.59218326, validate loss 1.66854710, acc 0.50200003\n",
      "42.0%, Epoch 20, train loss 1.59070280, validate loss 1.66882423, acc 0.49800003\n",
      "44.0%, Epoch 21, train loss 1.58616161, validate loss 1.66705913, acc 0.49400002\n",
      "46.0%, Epoch 22, train loss 1.58218494, validate loss 1.66416127, acc 0.50200003\n",
      "48.0%, Epoch 23, train loss 1.58037298, validate loss 1.66394083, acc 0.49800003\n",
      "50.0%, Epoch 24, train loss 1.58074346, validate loss 1.66424042, acc 0.49800003\n",
      "52.0%, Epoch 25, train loss 1.57691881, validate loss 1.66191300, acc 0.50800002\n",
      "54.0%, Epoch 26, train loss 1.57517478, validate loss 1.66172037, acc 0.50400001\n",
      "56.0%, Epoch 27, train loss 1.57484971, validate loss 1.66109100, acc 0.50600004\n",
      "58.0%, Epoch 28, train loss 1.57088574, validate loss 1.65993165, acc 0.51200002\n",
      "60.0%, Epoch 29, train loss 1.56941770, validate loss 1.65928123, acc 0.51200002\n",
      "62.0%, Epoch 30, train loss 1.56765871, validate loss 1.65923808, acc 0.51200002\n",
      "64.0%, Epoch 31, train loss 1.56343006, validate loss 1.65904203, acc 0.50800002\n",
      "66.0%, Epoch 32, train loss 1.56799592, validate loss 1.65809303, acc 0.51200002\n",
      "68.0%, Epoch 33, train loss 1.56577636, validate loss 1.65803233, acc 0.51400000\n",
      "70.0%, Epoch 34, train loss 1.56401603, validate loss 1.65774115, acc 0.51400000\n",
      "72.0%, Epoch 35, train loss 1.56637301, validate loss 1.65733885, acc 0.51400000\n",
      "74.0%, Epoch 36, train loss 1.56252660, validate loss 1.65720184, acc 0.51200002\n",
      "76.0%, Epoch 37, train loss 1.56490290, validate loss 1.65685475, acc 0.51600003\n",
      "78.0%, Epoch 38, train loss 1.55975963, validate loss 1.65674398, acc 0.51600003\n",
      "80.0%, Epoch 39, train loss 1.56032827, validate loss 1.65667762, acc 0.51400000\n",
      "82.0%, Epoch 40, train loss 1.56304057, validate loss 1.65627819, acc 0.51600003\n",
      "84.0%, Epoch 41, train loss 1.56146267, validate loss 1.65608767, acc 0.52000004\n",
      "86.0%, Epoch 42, train loss 1.56004919, validate loss 1.65583528, acc 0.51800001\n",
      "88.0%, Epoch 43, train loss 1.55886644, validate loss 1.65564482, acc 0.51800001\n",
      "90.0%, Epoch 44, train loss 1.55863889, validate loss 1.65558718, acc 0.51800001\n",
      "92.0%, Epoch 45, train loss 1.55995177, validate loss 1.65560438, acc 0.51800001\n",
      "94.0%, Epoch 46, train loss 1.55955478, validate loss 1.65566077, acc 0.51600003\n",
      "96.0%, Epoch 47, train loss 1.56091400, validate loss 1.65569749, acc 0.52000004\n",
      "98.0%, Epoch 48, train loss 1.55894426, validate loss 1.65579102, acc 0.51800001\n",
      "100.0%, Epoch 49, train loss 1.55958138, validate loss 1.65568479, acc 0.51800001\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "acc_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    valid_loss, valid_acc = validate(model, dev_dataloader, criterion, optimizer, epoch)\n",
    "    if valid_acc > best_acc:\n",
    "        acc_model = deepcopy(model)\n",
    "        best_acc = valid_acc\n",
    "    print(\"{}%, Epoch {}, train loss {:.8f}, validate loss {:.8f}, acc {:.8f}\".format(100*(epoch+1)/n_epochs,str(epoch).zfill(2),train_loss,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448fcd2-f8da-4307-ac75-f94a7f8fa1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
