{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c10dd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score \n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def load_train_data(file):\n",
    "    test_word_list = []\n",
    "    test_tag_list = []\n",
    "    test_index_list = []\n",
    "    \n",
    "    word_list = []\n",
    "    tag_list = []\n",
    "    index_list = []\n",
    "    index = 1\n",
    "    count = 629\n",
    "    for line in file:\n",
    "        if line != \"\\n\":\n",
    "            tmp = line.replace(\"\\n\",\"\").split()\n",
    "            word = tmp[0]\n",
    "            tag = tmp[1]\n",
    "            if count >= 0 :\n",
    "                word_list.append(word)\n",
    "                tag_list.append(tag)\n",
    "                index_list.append(index)\n",
    "            else:\n",
    "                test_word_list.append(word)\n",
    "                test_tag_list.append(tag)\n",
    "                test_index_list.append(index)\n",
    "            index = index + 1\n",
    "            \n",
    "        else:\n",
    "            index = 1\n",
    "            count = count - 1\n",
    "            \n",
    "    return index_list, word_list, tag_list, test_index_list, test_word_list,test_tag_list\n",
    "\n",
    "with open(\"pos_fixed.txt\", \"r\") as f:\n",
    "    train_data = f.readlines()\n",
    "test_index_lists, test_word_lists, test_tag_lists,train_index_lists, train_word_lists, train_tag_lists = load_train_data(train_data)\n",
    "#print(train_word_lists)\n",
    "#print(train_index_lists)\n",
    "\n",
    "data_dict = {\"index\": train_index_lists, \"word\": train_word_lists, \"tag\" : train_tag_lists}\n",
    "test_data_dict = {\"index\": test_index_lists, \"word\": test_word_lists, \"tag\" : test_tag_lists}\n",
    "test_data=pd.DataFrame(data_dict)\n",
    "train_data = pd.DataFrame(test_data_dict)\n",
    "# print(test_data)\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652a22f",
   "metadata": {},
   "source": [
    "## Enlarge the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94d50b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path_2 = \"train\"\n",
    "train_data_2 = pd.read_table(train_data_path_2, header =None, names = [\"index\", \"word\", \"tag\"])\n",
    "train_data_2 = train_data_2.dropna()\n",
    "\n",
    "train_data_path_3 = \"dev\"\n",
    "train_data_3 = pd.read_table(train_data_path_3, header =None, names = [\"index\", \"word\", \"tag\"])\n",
    "train_data_3 = train_data_3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "849a351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = [train_data,train_data_2, train_data_3]\n",
    "train_data = pd.concat(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af0e9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4065b368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@paulwalk</td>\n",
       "      <td>USR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>It</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>'s</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>view</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131763</th>\n",
       "      <td>13</td>\n",
       "      <td>join</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131764</th>\n",
       "      <td>14</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131765</th>\n",
       "      <td>15</td>\n",
       "      <td>winning</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131766</th>\n",
       "      <td>16</td>\n",
       "      <td>bidder</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131767</th>\n",
       "      <td>17</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056066 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       word  tag\n",
       "0           1  @paulwalk  USR\n",
       "1           2         It  PRP\n",
       "2           3         's  VBZ\n",
       "3           4        the   DT\n",
       "4           5       view   NN\n",
       "...       ...        ...  ...\n",
       "131763     13       join   VB\n",
       "131764     14        the   DT\n",
       "131765     15    winning  VBG\n",
       "131766     16     bidder   NN\n",
       "131767     17          .    .\n",
       "\n",
       "[1056066 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d802ba",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7272c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = train_data.groupby(\"word\")[\"index\"].count().reset_index(name = \"occurrences\")\n",
    "train_data = pd.merge(train_data, occurrences, how='left', on = \"word\")\n",
    "# backup\n",
    "backup_train_data_raw = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b5388c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsFloat(s):\n",
    "    pattern = '^-?\\d+\\.?\\d*$'  \n",
    "    match = re.match(pattern, s)\n",
    "    return match != None\n",
    "  \n",
    "def IsDigit(s):\n",
    "    if s[1:len(s)-1].replace(\",\",\"\").isdigit():\n",
    "        return True\n",
    "    elif IsFloat(s[1:len(s)-1]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def contain_Digit(word):\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732583c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_unk(train_data, threshold):\n",
    "  # count_unk = train_data[train_data[\"occurrences\"] < threshold].sum()[\"occurrences\"]\n",
    "\n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: \"http\" in x)), \"<unk_http>\")\n",
    "\n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[0] == \"@\")), \"<unk_usr>\")\n",
    "    \n",
    "    train_data[\"word\"] = train_data[\"word\"]\\\n",
    "                      .mask((train_data[\"occurrences\"] < threshold)\n",
    "                            & (train_data[\"word\"].apply(lambda x: x.replace(\",\",\"\").isdigit())), \"<unk_num>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"]\\\n",
    "                      .mask((train_data[\"occurrences\"] < threshold)\n",
    "                            & (train_data[\"word\"].apply(lambda x: IsFloat(x))), \"<unk_num>\")\n",
    "    \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x.isupper())), \"<unk_upper>\")\n",
    "    \n",
    "#     train_data[\"word\"] = train_data[\"word\"]\\\n",
    "#                       .mask((train_data[\"occurrences\"] < threshold)\n",
    "#                             & (train_data[\"word\"].apply(lambda x: contain_Digit(x))), \"<unk_num>\")\n",
    "  \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: \"-\" in x)), \"<unk_jj>\")\n",
    "  \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                                & (train_data[\"word\"].apply(lambda x: x.istitle())), \"<unk_title>\")\n",
    "  # train_data[\"word\"] = train_data[\"word\"]\\\n",
    "  #                     .mask((train_data[\"occurrences\"] < threshold)\n",
    "  #                           & (train_data[\"word\"].apply(lambda x: IsDigit(x))), \"<unk_num>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[len(x)-2:] == \"ed\")), \"<unk_ed>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[len(x)-3:] == \"ing\")), \"<unk_ing>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[len(x)-1:] == \"s\")), \"<unk_s>\")\n",
    " \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask( (train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x not in [\"<unk_jj>\", \"<unk_title>\", \"<unk_s>\", \"<unk_upper>\"\n",
    "                                                                                              \"<unk_usr>\", \"<unk_num>\", \"<unk_ed>\", \"<unk_ing>\", \"<unk_http>\"])), \"<unk>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "783c08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_unk(train_data, threshold):\n",
    "  # count_unk = train_data[train_data[\"occurrences\"] < threshold].sum()[\"occurrences\"]\n",
    "                        \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[0] == \"@\")), \"<unk_usr>\")\n",
    "    \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: \"http\" in x)), \"<unk_http>\")\n",
    "    \n",
    "    train_data[\"word\"] = train_data[\"word\"]\\\n",
    "                      .mask((train_data[\"occurrences\"] < threshold)\n",
    "                            & (train_data[\"word\"].apply(lambda x: x.replace(\",\",\"\").isdigit())), \"<unk_num>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"]\\\n",
    "                      .mask((train_data[\"occurrences\"] < threshold)\n",
    "                            & (train_data[\"word\"].apply(lambda x: IsFloat(x))), \"<unk_num>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"]\\\n",
    "                      .mask((train_data[\"occurrences\"] < threshold)\n",
    "                            & (train_data[\"word\"].apply(lambda x: contain_Digit(x))), \"<unk_num>\")\n",
    "    \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x.isupper())), \"<unk_upper>\")\n",
    "  \n",
    "  \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: \"-\" in x)), \"<unk_jj>\")\n",
    "  \n",
    "   \n",
    "  \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                                & (train_data[\"word\"].apply(lambda x: x.istitle())), \"<unk_title>\")\n",
    "  # train_data[\"word\"] = train_data[\"word\"]\\\n",
    "  #                     .mask((train_data[\"occurrences\"] < threshold)\n",
    "  #                           & (train_data[\"word\"].apply(lambda x: IsDigit(x))), \"<unk_num>\")\n",
    "    \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[len(x)-2:] == \"ed\")), \"<unk_ed>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[len(x)-3:] == \"ing\")), \"<unk_ing>\")\n",
    "    train_data[\"word\"] = train_data[\"word\"].mask((train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x[len(x)-1:] == \"s\")), \"<unk_s>\")\n",
    " \n",
    "    train_data[\"word\"] = train_data[\"word\"].mask( (train_data[\"occurrences\"] < threshold) \n",
    "                                              & (train_data[\"word\"].apply(lambda x: x not in [\"<unk_jj>\", \"<unk_title>\", \"<unk_s>\", \"<unk_upper>\",\n",
    "                                                                                              \"<unk_usr>\", \"<unk_num>\", \"<unk_ed>\", \"<unk_ing>\", \"<unk_http>\"])), \"<unk>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43e5a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "deal_unk(train_data, threshold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcdbc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "backup_train_data_unk = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "137fdb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99c6af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "special = train_data[train_data[\"occurrences\"] < threshold][[\"word\",\"occurrences\"]]\n",
    "special = special.groupby(\"word\")[\"occurrences\"].sum().reset_index(name = \"occurrences\")\n",
    "special.sort_values(\"occurrences\", inplace = True, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d3906cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unk = sum(special[\"occurrences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33011001",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_remain = train_data[train_data[\"occurrences\"] >= threshold][[\"word\",\"occurrences\"]]\n",
    "train_data_remain.sort_values(\"occurrences\", inplace = True, ascending = False)\n",
    "train_data_remain = train_data_remain.drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfd27a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.concat([special,train_data_remain ]).reset_index(drop=True)\n",
    "vocabulary[\"index\"] = vocabulary.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "457690e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "vocabulary[[\"word\", \"index\", \"occurrences\"]].to_csv(\"vocab.txt\", sep='\\t', index=None, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2a6b614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total size of the vocabulary is: 25545\n",
      "The total occurrences of all types of the special token '<unk>' is: 22985\n",
      "The total size of the '<unk_usr>' is: 343\n",
      "The total size of the '<unk_num>' is: 3607\n",
      "The total size of the '<unk_ed>' is: 960\n",
      "The total size of the '<unk_ing>' is: 849\n",
      "The total size of the '<unk_s>' is: 2144\n",
      "The total size of the '<unk_title>' is: 6058\n",
      "The total size of the '<unk_jj>' is: 3291\n",
      "The total size of the '<unk_http>' is: 126\n",
      "The total size of the '<unk_upper>' is: 1118\n",
      "The total size of the '<unk>' is: 4489\n"
     ]
    }
   ],
   "source": [
    "# information about dictionary\n",
    "# total size of vocabulary\n",
    "print(\"The total size of the vocabulary is:\", len(vocabulary))\n",
    "#total occurrences of the special\n",
    "print(\"The total occurrences of all types of the special token \\'<unk>\\' is:\", total_unk)\n",
    "# sperated information about <unk>\n",
    "\n",
    "print(\"The total size of the \\'<unk_usr>\\' is:\", int(special[special[\"word\"] == \"<unk_usr>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_num>\\' is:\", int(special[special[\"word\"] == \"<unk_num>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_ed>\\' is:\", int(special[special[\"word\"] == \"<unk_ed>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_ing>\\' is:\", int(special[special[\"word\"] == \"<unk_ing>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_s>\\' is:\", int(special[special[\"word\"] == \"<unk_s>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_title>\\' is:\", int(special[special[\"word\"] == \"<unk_title>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_jj>\\' is:\", int(special[special[\"word\"] == \"<unk_jj>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_http>\\' is:\", int(special[special[\"word\"] == \"<unk_http>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk_upper>\\' is:\", int(special[special[\"word\"] == \"<unk_upper>\"].occurrences))\n",
    "print(\"The total size of the \\'<unk>\\' is:\", int(special[special[\"word\"] == \"<unk>\"].occurrences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170d742",
   "metadata": {},
   "source": [
    "# Calculate transition and emssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14682d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = backup_train_data_unk.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1561bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_s = train_data.groupby(by=train_data[\"tag\"])[\"index\"].count().reset_index(name = \"count_s\")\n",
    "count_s.columns=['s',\"count_s\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff413c8c",
   "metadata": {},
   "source": [
    "## Trainsition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e3f55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count(s-->s')\n",
    "train_data = backup_train_data_unk.copy()\n",
    "train_data[\"tag_2\"] = train_data[\"tag\"][1:].reset_index(drop=True)\n",
    "train_data[\"tag_2_index\"] = train_data[\"index\"][1:].reset_index(drop=True)\n",
    "train_data.loc[train_data[\"tag_2_index\"] == 1.0, \"tag_2\"] = None\n",
    "train_data[\"pair_t\"] = train_data[\"tag\"] + \" \" + train_data[\"tag_2\"]\n",
    "\n",
    "count_pair_t = train_data.groupby(by=train_data[\"pair_t\"])[\"index\"].count().reset_index(name = \"count_pair_t\").dropna()\n",
    "count_pair_t[\"s\"] = count_pair_t[\"pair_t\"].apply(lambda x: x.split()[0])\n",
    "\n",
    "# compute transition\n",
    "transition = pd.merge(count_pair_t, count_s, how='left', on = \"s\")\n",
    "transition[\"value\"] = transition[\"count_pair_t\"]/transition[\"count_s\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9159b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition[\"pair\"] = transition[\"pair_t\"].apply(lambda x: str((str(x.split()[0]),str(x.split()[1]))))\n",
    "transition = transition[[\"pair\", \"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d341cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "transition = transition.set_index([\"pair\"])\n",
    "transition_dic = transition.to_dict(\"dic\")[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bd714",
   "metadata": {},
   "source": [
    "## emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fdaf1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count(s-->x)\n",
    "train_data = backup_train_data_unk.copy()\n",
    "train_data[\"pair_e\"]= train_data[\"tag\"] + \" \" + train_data[\"word\"]\n",
    "\n",
    "count_pair_e = train_data.groupby(by=train_data[\"pair_e\"])[\"index\"].count().reset_index(name = \"count_pair_e\").dropna()\n",
    "count_pair_e[\"s\"] = count_pair_e[\"pair_e\"].apply(lambda x: x.split()[0])\n",
    "\n",
    "# compute emission\n",
    "emission = pd.merge(count_pair_e, count_s, how='left', on = \"s\")\n",
    "emission[\"value\"] = emission[\"count_pair_e\"]/emission[\"count_s\"]  # round(, 4)\n",
    "emission[\"pair\"] = emission[\"pair_e\"].apply(lambda x: str((str(x.split()[0]),str(x.split()[1]))))\n",
    "# emission[\"pair\"] = emission[\"pair_e\"].apply(lambda x: tuple([str(x.split()[0]),str(x.split()[1])]))\n",
    "emission = emission[[\"pair\", \"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5118a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "emission = emission.set_index([\"pair\"])\n",
    "emission_dic = emission.to_dict(\"dic\")[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0188ea",
   "metadata": {},
   "source": [
    "## Save to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6e73d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_json = {}\n",
    "hmm_json[\"trainsition\"] = transition_dic\n",
    "hmm_json[\"emission\"] = emission_dic \n",
    "\n",
    "json_str = json.dumps(hmm_json)\n",
    "with open(\"hmm.json\", \"w\") as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f63bafdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1580 transition in HMM.\n",
      "There are 33740 emission in HMM.\n"
     ]
    }
   ],
   "source": [
    "# information about transition and emission\n",
    "print(\"There are\", len(transition), \"transition in HMM.\")\n",
    "print(\"There are\", len(emission), \"emission in HMM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af68a8",
   "metadata": {},
   "source": [
    "# Viterbi Decoding with HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "614663d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# compute first tag probability : t(s1)\n",
    "first_word = backup_train_data_unk.copy()\n",
    "first_word = first_word[first_word[\"index\"] == 1]\n",
    "sentence_num = len(first_word)\n",
    "tag_count = first_word.groupby(by=train_data[\"tag\"])[\"index\"].count().reset_index(name = \"count\")\n",
    "tag_count[\"t(s1)\"] = tag_count[\"count\"] / sentence_num\n",
    "tag_count = tag_count[[\"tag\", \"t(s1)\"] ].set_index([\"tag\"])\n",
    "first_word_t = tag_count.to_dict(\"dic\")[\"t(s1)\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b7f4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible tag dictionary\n",
    "word_tag = backup_train_data_unk.copy()\n",
    "def concat_func(x):\n",
    "    return pd.Series({\n",
    "        'tag': ' '.join(x['tag'].unique()).split(),\n",
    "    })\n",
    "word_tag = word_tag.groupby(word_tag[\"word\"]).apply(concat_func).reset_index()\n",
    "word_tag_dic = word_tag.set_index([\"word\"]).to_dict(\"dic\")[\"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "528c871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tag_dic[\"<unk_upper>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag dictionary\n",
    "tag_data = backup_train_data_unk.copy()\n",
    "tag_dic = tag_data.groupby(by=train_data[\"tag\"])[\"index\"].count().reset_index(name = \"count\")\n",
    "tag_dic = tag_dic.set_index([\"tag\"])\n",
    "tag_dic = tag_dic.to_dict(\"dic\")[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1687faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(sentence, first_word_t, transition_dic, emission_dic, word_tag_dic):\n",
    "    n = len(sentence)\n",
    "    path = {}\n",
    "    \n",
    "    # first word\n",
    "    probability = [{}]\n",
    "    x1 = sentence[0]\n",
    "    if x1 not in word_tag_dic:\n",
    "        if \"http\" in x1:\n",
    "            x1 = \"<unk_http>\"\n",
    "        elif x1[0] == \"@\":\n",
    "            x1 = \"<unk_usr>\"\n",
    "        elif x1.replace(\",\",\"\").isdigit() or IsFloat(x1) : #or contain_Digit(x1)\n",
    "            x1 = \"<unk_num>\"\n",
    "#         elif IsDigit(x1):\n",
    "#             x1 = \"<unk_num>\"\n",
    "        elif x1.isupper():\n",
    "            x1 = \"<unk_upper>\"\n",
    "        elif \"-\" in x1:\n",
    "            x1 = \"<unk_jj>\"\n",
    "        elif x1[len(x1)-2:] == \"ed\":\n",
    "            x1 = \"<unk_ed>\"\n",
    "        elif x1[len(x1)-3:] == \"ing\":\n",
    "            x1 = \"<unk_ing>\"\n",
    "        elif x1[len(x1)-1:] == \"s\":\n",
    "            x1 = \"<unk_s>\"\n",
    "        elif x1.istitle():\n",
    "            x1 = \"<unk_title>\"\n",
    "        elif IsFloat(x1):\n",
    "            x1 = \"<unk_num>\"\n",
    "        else:\n",
    "            x1 = \"<unk>\"\n",
    "    potential_tag_curr = word_tag_dic[x1]\n",
    "    # print(potential_tag_curr)\n",
    "    for state in potential_tag_curr:\n",
    "        if state not in first_word_t:\n",
    "            probability[0][state] = 0\n",
    "        elif str((state, x1)) not in emission_dic:\n",
    "            probability[0][state] = 0\n",
    "        else:   \n",
    "            probability[0][state] = first_word_t[state] * emission_dic[str((state, x1))]\n",
    "        path[state] = [state]\n",
    "    # print(cur_pro)\n",
    "    # print(path)\n",
    "    \n",
    "    # HMM\n",
    "    for i in range(1,n):\n",
    "        xi = sentence[i]\n",
    "        # print(xi)\n",
    "        if xi not in word_tag_dic:\n",
    "            if \"http\" in xi:\n",
    "                xi = \"<unk_http>\"\n",
    "            elif xi[0] == \"@\":\n",
    "                xi = \"<unk_usr>\"\n",
    "            elif xi.replace(\",\",\"\").isdigit():\n",
    "                xi = \"<unk_num>\"\n",
    "            elif IsFloat(xi) : # or contain_Digit(xi)\n",
    "                xi = \"<unk_num>\"\n",
    "            elif xi.isupper():\n",
    "                xi = \"<unk_upper>\"\n",
    "            \n",
    "#             elif IsDigit(xi):\n",
    "#                 xi = \"<unk_num>\"\n",
    "            elif \"-\" in xi:\n",
    "                xi = \"<unk_jj>\"\n",
    "            \n",
    "            elif xi[len(xi)-2:] == \"ed\":\n",
    "                xi = \"<unk_ed>\"\n",
    "            elif xi[len(xi)-3:] == \"ing\":\n",
    "                xi = \"<unk_ing>\"\n",
    "            elif xi[len(xi)-1:] == \"s\":\n",
    "                xi = \"<unk_s>\"\n",
    "            elif xi.istitle():\n",
    "                xi = \"<unk_title>\"\n",
    "            else:\n",
    "                xi = \"<unk>\"\n",
    "        new_path = {}\n",
    "        probability.append({}) \n",
    "        potential_tag_last = potential_tag_curr\n",
    "        # print(xi)\n",
    "        potential_tag_curr = word_tag_dic[xi]\n",
    "        # print(potential_tag_curr)\n",
    "        # print(potential_tag_last)\n",
    "\n",
    "        record_pro = {}\n",
    "        for state in potential_tag_curr:\n",
    "            for s in potential_tag_last:\n",
    "                if str((s,state)) not in transition_dic:\n",
    "                    record_pro[s] = 0\n",
    "                elif str((state, xi)) not in emission_dic:\n",
    "                    record_pro[s] = 0\n",
    "                else:\n",
    "                    record_pro[s] = probability[i-1][s]*transition_dic[str((s,state))]*emission_dic[str((state, xi))]\n",
    "            max_pro, last_state = max(zip(record_pro.values(), record_pro.keys()))\n",
    "            # print(state)\n",
    "            probability[i][state] = max_pro\n",
    "            new_path[state] = path[last_state] + [state]\n",
    "        path = new_path\n",
    "    \n",
    "    # find max probability\n",
    "    # print(path)\n",
    "    end_tag = potential_tag_curr\n",
    "    max_pro, end_state = max([(probability[n - 1][end], end) for end in end_tag])\n",
    "    return path[end_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8bdac",
   "metadata": {},
   "source": [
    "## Prediction on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34d4772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a844c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"next\"] = test_data['index'][1:].reset_index(drop = True)\n",
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6035cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_acc = {}\n",
    "sequence = \"\"\n",
    "tag = \"\"\n",
    "for index, row in test_data.iterrows():\n",
    "    if row['next'] is None or row['next'] != 1:\n",
    "        sequence = sequence + \" \" + row[\"word\"]\n",
    "        tag = tag + \" \" + row[\"tag\"]\n",
    "    else:\n",
    "        sequence = sequence + \" \" + row[\"word\"]\n",
    "        tag = tag + \" \" + row[\"tag\"]\n",
    "        test_data_acc[sequence] = tag\n",
    "        sequence = \"\"\n",
    "        tag = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d02f0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "acc_viterbi = AverageMeter()\n",
    "\n",
    "res = {}\n",
    "for sentence, tags in test_data_acc.items():\n",
    "    viterbi_pre = viterbi_decoding(sentence.split(), first_word_t, transition_dic, emission_dic, word_tag_dic)\n",
    "    res[sentence] = (viterbi_pre, tags.split())\n",
    "    acc = accuracy_score(tags.split(), viterbi_pre)\n",
    "    acc_viterbi.update(acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "008db6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Viterbi Decoding on the dev data is 81.40570512937838%\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of Viterbi Decoding on the dev data is {}%\".format(acc_viterbi.avg * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c3ce58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(res)\n",
    "with open(\"viterbi.out\", \"w\") as f:\n",
    "    for key, value in res.items():\n",
    "        sentence_list = key.split()\n",
    "        for i in range(len(sentence_list)):\n",
    "            f.write(str(i+1) + \"\\t\" + sentence_list[i] + \"\\t\" + value[1][i] +  \"\\t\" + value[0][i] +\"\\n\")\n",
    "        n = n - 1\n",
    "        if n != 0:\n",
    "            f.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d62bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
