{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c25b357-b804-4ce8-8942-0b59f6be9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2accd74-1cc5-415e-874f-46c4d7276196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as datasets\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4fa5c8-2641-45da-b210-85adacf22ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_class = ppb.DistilBertModel\n",
    "tokenizer_class = ppb.DistilBertTokenizer\n",
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13a0694-3370-4dae-8a79-16fd1b2d6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(file_path = \"./data/ISEAR.txt\"):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            y_x = lines.split(\"|\")\n",
    "            if len(y_x) < 2:\n",
    "                continue\n",
    "            train_y.append(y_x[0])\n",
    "            train_x.append(y_x[1])\n",
    "    return train_x, train_y\n",
    "\n",
    "def load_dev(file_path = \"./data/test.txt\"):\n",
    "    dev_x = []\n",
    "    dev_y = []\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            y_x = lines.split(\"|\")\n",
    "            if len(y_x) < 2:\n",
    "                continue\n",
    "            dev_y.append(y_x[0])\n",
    "            dev_x.append(y_x[1])\n",
    "    return dev_x, dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5844e8-0ada-4b84-9e95-3deeaaed6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_raw, train_y_raw = load_train()\n",
    "dev_x_raw, dev_y_raw = load_dev()\n",
    "\n",
    "y_count = {}\n",
    "for y in train_y_raw:\n",
    "    if y in y_count:\n",
    "        y_count[y] += 1\n",
    "    else:\n",
    "        y_count[y] = 1\n",
    "def sort_key(a):\n",
    "    return a[1]\n",
    "label_list = []\n",
    "for key in y_count:\n",
    "    item = y_count[key]\n",
    "    label_list.append((key,item))\n",
    "label_list.sort(reverse=True,key=sort_key)\n",
    "label2id = {}\n",
    "i = 0\n",
    "for label in label_list:\n",
    "    label2id[label[0]] = i\n",
    "    label2id[i] = label[0]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2591971-9b8d-45be-96e3-5541810cdba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joy': 0,\n",
       " 0: 'joy',\n",
       " 'sadness': 1,\n",
       " 1: 'sadness',\n",
       " 'anger': 2,\n",
       " 2: 'anger',\n",
       " 'fear': 3,\n",
       " 3: 'fear',\n",
       " 'shame': 4,\n",
       " 4: 'shame',\n",
       " 'disgust': 5,\n",
       " 5: 'disgust',\n",
       " 'guilt': 6,\n",
       " 6: 'guilt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b381d38-b281-40a9-b9e8-9d54efdf7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = []\n",
    "dev_y = []\n",
    "for label in train_y_raw:\n",
    "    train_y.append(label2id[label])\n",
    "for label in dev_y_raw:\n",
    "    dev_y.append(label2id[label])\n",
    "train_y = np.array(train_y)\n",
    "dev_y = np.array(dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141d3cb5-d5a7-4f96-b5a0-093256a9b6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b6816b-b622-4485-ba02-faa170a1bd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e8278e8-77a6-4af9-a7ca-4c85a05d3a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 5925, 3968, 2546, 1012, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"abc edf.\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb362bb7-9f7d-4cfa-a77d-409cb048cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence_list):\n",
    "    l = []\n",
    "    for i in range(len(sentence_list)):\n",
    "        s = sentence_list[i].lower()\n",
    "        s = tokenizer.encode(s, add_special_tokens=True)\n",
    "        l.append(s)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79e8a9f-c5d0-4b7c-97dd-957184b796e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tokenized = preprocess(train_x_raw)\n",
    "dev_x_tokenized = preprocess(dev_x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a298558b-3e5b-49dc-b648-256125a56343",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in train_x_tokenized:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in train_x_tokenized])\n",
    "padded_dev = np.array([i + [0]*(max_len-len(i)) for i in dev_x_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9ba2cd-bef4-4671-a2df-5a58c75af369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  2006,  2420, ...,     0,     0,     0],\n",
       "       [  101,  2296,  2051, ...,     0,     0,     0],\n",
       "       [  101,  2043,  1045, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  2023,  3277, ...,     0,     0,     0],\n",
       "       [  101,  2026, 18328, ...,     0,     0,     0],\n",
       "       [  101,  2197,  2621, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373add1b-c25e-41bf-a2ff-869e9db1d11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 195)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc42f608-1ce4-45e8-a106-60770e3e61f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 195)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c321c7-4b5b-4926-adbe-4c62c6f6c891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 195)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dff10e6d-9967-435c-b48d-94d8791b115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = []\n",
    "attention_mask_train = []\n",
    "for i in range(14):\n",
    "    padded_train.append(padded[500*i:500*(i+1)])\n",
    "    attention_mask_train.append(attention_mask[500*i:500*(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75a775a3-bc23-49bd-b502-3442e6e472fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12381661-2185-4cbc-9d6b-83dcee0f0d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 195)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94239e9-d178-4f25-9d8f-2e5ccfc8f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_list = []\n",
    "for i in range(14):\n",
    "    torch.cuda.empty_cache()\n",
    "    input_ids = torch.tensor(padded_train[i]).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask_train[i]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    feature = last_hidden_states[0][:,0,:].cpu().numpy()\n",
    "    train_feature_list.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef7d362f-4fe7-4d4b-b148-46ec1fcc698b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10286859, -0.13684382, -0.25244638, ..., -0.17019261,\n",
       "         0.18526137,  0.08419316],\n",
       "       [ 0.05663628,  0.09149715, -0.141327  , ..., -0.07937766,\n",
       "         0.26432273,  0.27011317],\n",
       "       [-0.10499352, -0.0638291 , -0.17506857, ..., -0.051651  ,\n",
       "         0.24059753,  0.22042893],\n",
       "       ...,\n",
       "       [-0.04100357, -0.27688867, -0.09601278, ..., -0.10334397,\n",
       "         0.28090087,  0.21965046],\n",
       "       [ 0.08131988, -0.13469708,  0.08270323, ...,  0.066988  ,\n",
       "         0.29606506,  0.3803135 ],\n",
       "       [-0.17046656, -0.56751305, -0.24231999, ..., -0.0667112 ,\n",
       "         0.43118402, -0.05132144]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5b9e302-98f8-4dcf-8081-659d64cdd223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32313451-e898-4531-97ad-5241f51dfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask_dev = np.where(padded_dev != 0, 1, 0)\n",
    "input_ids_dev = torch.tensor(padded_dev).to(device)\n",
    "attention_mask_dev = torch.tensor(attention_mask_dev).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids_dev, attention_mask=attention_mask_dev)\n",
    "dev_x = last_hidden_states[0][:,0,:].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da1b80ed-eeae-4a03-a46b-c29900474081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f267d5a-1d0c-41a0-984c-cbc5d6c02b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((7000,768))\n",
    "np_i = 0\n",
    "for i in range(14): \n",
    "    for vec in train_feature_list[i]:\n",
    "        train_x[np_i] = vec\n",
    "        np_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7315c2e-7897-489b-8ee8-8afc61cf4f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc57303a-becb-45ee-9c7e-f60b8e86ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(datasets.Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f24ff3a8-1e05-423b-bf27-ea078537bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).long())\n",
    "dev_dataset = ClassifierDataset(torch.from_numpy(dev_x).float(), torch.from_numpy(dev_y).long())\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eec8dd52-a436-4630-bb76-9fe14a93ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(768, 512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, 7) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fcaa51a0-4d0c-40b2-969c-7d6a5d7151a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "class average_meter(object):\n",
    "    '''Computes and stores the average and current value\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b05ba58-c9a0-4f7d-ae0f-7afc6e656916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, train_loader, criterion, optimizer):\n",
    "    train_loss = average_meter()\n",
    "    classifier.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = classifier(x)\n",
    "        \n",
    "        loss = criterion(pred, y)\n",
    "        train_loss.update(loss.item(),x.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss.avg\n",
    "\n",
    "def validate(classifier, dev_loader, criterion, optimizer):\n",
    "    valid_loss = average_meter()\n",
    "    valid_acc = average_meter()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dev_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = classifier(x)\n",
    "            loss = criterion(pred, y)\n",
    "            acc = multi_acc(pred, y)\n",
    "            valid_loss.update(loss.item(),x.size(0))\n",
    "            valid_acc.update(acc.item(),x.size(0))\n",
    "    return valid_loss.avg, valid_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29f6a5a2-8ee7-460f-b743-a1701fec2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MulticlassClassification()\n",
    "classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "08ced0e5-93cd-41fd-855d-68fafbe850f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0%, Epoch 00, train loss 1.48977905, validate loss 1.56709671, acc 47.00000000\n",
      "4.0%, Epoch 01, train loss 1.22027572, validate loss 1.35392356, acc 51.00000000\n",
      "6.0%, Epoch 02, train loss 1.22483490, validate loss 1.33604479, acc 51.00000000\n",
      "8.0%, Epoch 03, train loss 1.21758670, validate loss 1.33285093, acc 51.00000000\n",
      "10.0%, Epoch 04, train loss 1.21962523, validate loss 1.33250916, acc 51.00000000\n",
      "12.0%, Epoch 05, train loss 1.22171966, validate loss 1.33206308, acc 51.00000000\n",
      "14.0%, Epoch 06, train loss 1.21761187, validate loss 1.33174908, acc 51.00000000\n",
      "16.0%, Epoch 07, train loss 1.22062011, validate loss 1.33170497, acc 51.00000000\n",
      "18.0%, Epoch 08, train loss 1.22306282, validate loss 1.33197606, acc 51.00000000\n",
      "20.0%, Epoch 09, train loss 1.21610779, validate loss 1.33187973, acc 51.00000000\n",
      "22.0%, Epoch 10, train loss 1.22112705, validate loss 1.33153236, acc 51.00000000\n",
      "24.0%, Epoch 11, train loss 1.22545358, validate loss 1.33171606, acc 51.00000000\n",
      "26.0%, Epoch 12, train loss 1.22178890, validate loss 1.33157933, acc 51.00000000\n",
      "28.0%, Epoch 13, train loss 1.22349246, validate loss 1.33194149, acc 51.00000000\n",
      "30.0%, Epoch 14, train loss 1.22016018, validate loss 1.33179092, acc 51.00000000\n",
      "32.0%, Epoch 15, train loss 1.22030089, validate loss 1.33195949, acc 51.00000000\n",
      "34.0%, Epoch 16, train loss 1.22248595, validate loss 1.33197379, acc 51.00000000\n",
      "36.0%, Epoch 17, train loss 1.21972569, validate loss 1.33188808, acc 51.00000000\n",
      "38.0%, Epoch 18, train loss 1.22357593, validate loss 1.33215392, acc 51.00000000\n",
      "40.0%, Epoch 19, train loss 1.21806491, validate loss 1.33174503, acc 51.00000000\n",
      "42.0%, Epoch 20, train loss 1.21653874, validate loss 1.33174396, acc 51.00000000\n",
      "44.0%, Epoch 21, train loss 1.22495967, validate loss 1.33161736, acc 51.00000000\n",
      "46.0%, Epoch 22, train loss 1.21465568, validate loss 1.33195567, acc 51.00000000\n",
      "48.0%, Epoch 23, train loss 1.22065758, validate loss 1.33227348, acc 51.00000000\n",
      "50.0%, Epoch 24, train loss 1.22127571, validate loss 1.33205378, acc 51.00000000\n",
      "52.0%, Epoch 25, train loss 1.22332712, validate loss 1.33200276, acc 51.00000000\n",
      "54.0%, Epoch 26, train loss 1.22187246, validate loss 1.33219767, acc 51.00000000\n",
      "56.0%, Epoch 27, train loss 1.21902020, validate loss 1.33205378, acc 51.00000000\n",
      "58.0%, Epoch 28, train loss 1.22499073, validate loss 1.33192933, acc 51.00000000\n",
      "60.0%, Epoch 29, train loss 1.22341783, validate loss 1.33191848, acc 51.00000000\n",
      "62.0%, Epoch 30, train loss 1.21619156, validate loss 1.33209169, acc 51.00000000\n",
      "64.0%, Epoch 31, train loss 1.22054522, validate loss 1.33200562, acc 51.00000000\n",
      "66.0%, Epoch 32, train loss 1.22090798, validate loss 1.33165264, acc 51.00000000\n",
      "68.0%, Epoch 33, train loss 1.22277677, validate loss 1.33206439, acc 51.00000000\n",
      "70.0%, Epoch 34, train loss 1.21975291, validate loss 1.33199430, acc 51.00000000\n",
      "72.0%, Epoch 35, train loss 1.22486874, validate loss 1.33178270, acc 51.00000000\n",
      "74.0%, Epoch 36, train loss 1.21705014, validate loss 1.33191454, acc 51.00000000\n",
      "76.0%, Epoch 37, train loss 1.21590146, validate loss 1.33216846, acc 51.00000000\n",
      "78.0%, Epoch 38, train loss 1.22053699, validate loss 1.33213198, acc 51.00000000\n",
      "80.0%, Epoch 39, train loss 1.21946016, validate loss 1.33166623, acc 51.00000000\n",
      "82.0%, Epoch 40, train loss 1.22201933, validate loss 1.33174181, acc 51.00000000\n",
      "84.0%, Epoch 41, train loss 1.22273266, validate loss 1.33197975, acc 51.00000000\n",
      "86.0%, Epoch 42, train loss 1.21620676, validate loss 1.33216107, acc 51.00000000\n",
      "88.0%, Epoch 43, train loss 1.21714729, validate loss 1.33197296, acc 51.00000000\n",
      "90.0%, Epoch 44, train loss 1.22154514, validate loss 1.33203554, acc 51.00000000\n",
      "92.0%, Epoch 45, train loss 1.22242566, validate loss 1.33234596, acc 51.00000000\n",
      "94.0%, Epoch 46, train loss 1.21712831, validate loss 1.33212268, acc 51.00000000\n",
      "96.0%, Epoch 47, train loss 1.21805827, validate loss 1.33138716, acc 51.00000000\n",
      "98.0%, Epoch 48, train loss 1.22227725, validate loss 1.33172929, acc 51.00000000\n",
      "100.0%, Epoch 49, train loss 1.21742218, validate loss 1.33201635, acc 51.00000000\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "best_acc = 0\n",
    "acc_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = train(classifier, train_dataloader, criterion, optimizer)\n",
    "    torch.cuda.empty_cache()\n",
    "    valid_loss, valid_acc = validate(classifier, dev_dataloader, criterion, optimizer)\n",
    "    if valid_acc > best_acc:\n",
    "        classifier = deepcopy(classifier)\n",
    "        best_acc = valid_acc\n",
    "    print(\"{}%, Epoch {}, train loss {:.8f}, validate loss {:.8f}, acc {:.8f}\".format(100*(epoch+1)/n_epochs,str(epoch).zfill(2),train_loss,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5807c18-4062-487f-a17d-a730101ae754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
