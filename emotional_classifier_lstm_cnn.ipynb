{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b16370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ed4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class average_meter(object):\n",
    "    '''Computes and stores the average and current value\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def load_glove(path = 'GloVe.json'):\n",
    "    with open('GloVe.json','r',encoding='utf8')as fp:\n",
    "        json_data = json.load(fp)\n",
    "    return json_data\n",
    "\n",
    "def load_model(file_path = \"./data/glove.twitter.27B.200d.txt\"):\n",
    "    glove = {}\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            items = lines.split()\n",
    "            if len(items) != 201:\n",
    "                continue\n",
    "            else:\n",
    "                word_vector = []\n",
    "                for i in range(1,201):\n",
    "                    word_vector.append(float(items[i]))\n",
    "                glove[items[0]] = word_vector\n",
    "    UNK = \"< UNK >\"\n",
    "    glove[UNK] = np.random.uniform(-0.25, 0.25, 200).tolist()\n",
    "    return glove\n",
    "\n",
    "def load_train(file_path = \"./data/ISEAR.txt\"):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            y_x = lines.split(\"|\")\n",
    "            train_y.append(y_x[0])\n",
    "            train_x.append(y_x[1])\n",
    "    return train_x, train_y\n",
    "\n",
    "def load_dev(file_path = \"./data/test.txt\"):\n",
    "    dev_x = []\n",
    "    dev_y = []\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        for lines in f:\n",
    "            y_x = lines.split(\"|\")\n",
    "            dev_y.append(y_x[0])\n",
    "            dev_x.append(y_x[2])\n",
    "    return dev_x, dev_y\n",
    "\n",
    "def split_train_test(data_X, data_Y, test_ratio):\n",
    "    \n",
    "    #combined_lsts = list(zip(data_X, data_Y))\n",
    "    #random.shuffle(combined_lsts)\n",
    "    test_set_size = int(len(data_X) * test_ratio)\n",
    "    #data_X, data_Y = list(zip(*combined_lsts))\n",
    "    train_x_raw = data_X[:test_set_size]\n",
    "    train_y_raw = data_Y[:test_set_size]\n",
    "    dev_x_raw = data_X[test_set_size:]\n",
    "    dev_y_raw = data_Y[test_set_size:]\n",
    "\n",
    "    return train_x_raw, train_y_raw, dev_x_raw, dev_y_raw\n",
    "\n",
    "\n",
    "contractions_dict = {\n",
    "    \"i'm\" : \"i am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"i'd\" : \"i would\",\n",
    "    \"i've\" : \"i have\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"that'll\" : \"that will\",\n",
    "    \"that'd\" : \"that would\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what'd\" : \"what would\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"where'll\" : \"where will\",\n",
    "    \"where'd\" : \"where would\",\n",
    "    \"when's\" : \"when is\",\n",
    "    \"when'll\" : \"when will\",\n",
    "    \"when'd\" : \"when would\",\n",
    "    \"why's\" : \"why is\",\n",
    "    \"why'll\" : \"why will\",\n",
    "    \"why'd\" : \"why would\",\n",
    "    \"how's\" : \"how is\",\n",
    "    \"how'll\" : \"how will\",\n",
    "    \"how'd\" : \"how would\",\n",
    "    \"would've\" : \"would have\",\n",
    "    \"should've\" : \"should have\",\n",
    "    \"could've\" : \"could have\",\n",
    "    \"might've\" : \"might have\",\n",
    "    \"must've\" : \"must have\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"wasn't\" : \"was not\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\"\n",
    "}\n",
    "\n",
    "def contractionfunction(s):\n",
    "    if s in contractions_dict.keys():\n",
    "        return contractions_dict[s]\n",
    "    return s\n",
    "\n",
    "def preprocess(sentence_list):\n",
    "    stop = stopwords.words('english')\n",
    "    char_replace = {\",\",\".\",\"/\",\";\",\"'\",\"[\",\"]\",\"\\\\\",\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"-\",\"_\",\"=\",\"+\",\"<\",\">\",\"?\",\":\",\"\\\"\",\"{\",\"}\",\"|\"}\n",
    "    for i in range(len(sentence_list)):\n",
    "        sentence_list[i] = sentence_list[i].lower()\n",
    "        for char in char_replace:\n",
    "            if char in sentence_list[i]:\n",
    "                sentence_list[i] = sentence_list[i].replace(char, \" \")\n",
    "        sentence_list[i] = ' '.join([word for word in sentence_list[i].split() if word not in (stop)])\n",
    "        sentence_list[i] = sentence_list[i].split()\n",
    "    return sentence_list\n",
    "\n",
    "def sort_key(a):\n",
    "    return a[1]\n",
    "\n",
    "def generate_label2id(y_data):\n",
    "    y_count = {}\n",
    "    label_list = []\n",
    "    for y in y_data:\n",
    "        if y in y_count:\n",
    "            y_count[y] += 1\n",
    "        else:\n",
    "            y_count[y] = 1\n",
    "    \n",
    "    for key in y_count:\n",
    "        item = y_count[key]\n",
    "        label_list.append((key,item))\n",
    "    label_list.sort(reverse=True,key=sort_key)\n",
    "\n",
    "    label2id = {}\n",
    "    i = 0\n",
    "    for label in label_list:\n",
    "        label2id[label[0]] = i\n",
    "        label2id[i] = label[0]\n",
    "        i+=1\n",
    "    return label2id\n",
    "\n",
    "def get_type_glove(unk):\n",
    "    digits = 0\n",
    "    for c in unk:\n",
    "        if c.isdigit():\n",
    "            digits += 1\n",
    "    df = digits/len(unk)\n",
    "    if unk.isdigit():\n",
    "        return 1.0\n",
    "    elif df > 0.5:\n",
    "        return 2.0\n",
    "    elif digits>0:\n",
    "        return 3.0\n",
    "    else:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe59c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as datasets\n",
    "\n",
    "class emotion_dataset(datasets.Dataset):\n",
    "    \n",
    "    def __init__(self, word_lists, label_lists):\n",
    "        self.word_lists = word_lists\n",
    "        self.label_lists = label_lists\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.word_lists[index], self.label_lists[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e3da053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, label2id, GloVe):\n",
    "    b_size = len(data)\n",
    "    x_tensor = torch.zeros(b_size, 80, 201)\n",
    "    y_tensor = torch.zeros(b_size).long()\n",
    "    \n",
    "    for b_index in range(b_size):\n",
    "        x = data[b_index][0]\n",
    "        y = data[b_index][1]\n",
    "        for xy_index in range(len(x)):\n",
    "            word = x[xy_index]\n",
    "            if word in GloVe:\n",
    "                x_vector = deepcopy(GloVe[word])\n",
    "                one_more = get_type_glove(word)\n",
    "                x_vector.append(one_more)\n",
    "                _x = torch.FloatTensor(x_vector)\n",
    "                x_tensor[b_index][xy_index] = _x\n",
    "            else:\n",
    "                unk_vector = deepcopy(GloVe[\"< UNK >\"])\n",
    "                one_more = get_type_glove(word)\n",
    "                unk_vector.append(one_more)\n",
    "                _unk = torch.FloatTensor(unk_vector)\n",
    "                x_tensor[b_index][xy_index] = _unk\n",
    "        y_tensor[b_index] = label2id[y]\n",
    "    data_len = []\n",
    "    for xy in data:\n",
    "        data_len.append(len(xy[0]))\n",
    "    return x_tensor, y_tensor, data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8824bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Glove dictionary\n"
     ]
    }
   ],
   "source": [
    "print('loading Glove dictionary')\n",
    "#Glove = load_glove()\n",
    "Glove = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce8050a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence_list):\n",
    "    stop = stopwords.words('english')\n",
    "    char_replace = {\",\",\".\",\"/\",\";\",\"'\",\"[\",\"]\",\"\\\\\",\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"*\",\"(\",\")\",\"_\",\"=\",\"+\",\"<\",\">\",\"?\",\":\",\"\\\"\",\"{\",\"}\",\"|\"}\n",
    "    for i in range(len(sentence_list)):\n",
    "        sentence_list[i] = sentence_list[i].lower()\n",
    "        for char in char_replace:\n",
    "            if char in sentence_list[i]:\n",
    "                sentence_list[i] = sentence_list[i].replace(char, \" \")\n",
    "        if '-' in sentence_list[i]:\n",
    "            temp = sentence_list[i].split()\n",
    "            for word in temp: \n",
    "                if '-' in word and word not in Glove:\n",
    "                    sentence_list[i] = sentence_list[i].replace('-', \" \")\n",
    "        sentence_list[i] = ' '.join([word for word in sentence_list[i].split() if word not in (stop)])\n",
    "        sentence_list[i] = sentence_list[i].split()\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "850e7272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train and test data\n",
      "preparing Dataset\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "lr = 0.001\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "DEV_BATCH_SIZE = 96\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('loading train and test data')\n",
    "train_x_raw, train_y_raw = load_train(file_path = \"./data/train.txt\")\n",
    "dev_x_raw, dev_y_raw = load_train(file_path = \"./data/test.txt\")\n",
    "train_x_raw = preprocess(train_x_raw)\n",
    "dev_x_raw = preprocess(dev_x_raw)\n",
    "\n",
    "\n",
    "label2id = generate_label2id(train_y_raw)\n",
    "\n",
    "train_y = []\n",
    "dev_y = []\n",
    "for label in train_y_raw:\n",
    "    train_y.append(label2id[label])\n",
    "for label in dev_y_raw:\n",
    "    dev_y.append(label2id[label])\n",
    "\n",
    "print('preparing Dataset')\n",
    "train_dataset = emotion_dataset(train_x_raw,train_y_raw)\n",
    "dev_dataset = emotion_dataset(dev_x_raw,dev_y_raw)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, collate_fn= lambda x: data_process(x, label2id, Glove))\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=DEV_BATCH_SIZE, shuffle=False, collate_fn= lambda x: data_process(x, label2id, Glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87ba61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, criterion, optimizer, epoch, DEVICE):\n",
    "    train_loss = average_meter()\n",
    "    model.train()\n",
    "    for i in train_loader:\n",
    "        x = i[0].to(DEVICE)\n",
    "        y = i[1].to(DEVICE)\n",
    "        l = i[2]\n",
    "        pred = model.forward(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(pred, y).to(DEVICE)\n",
    "        train_loss.update(loss.item(),x.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss.avg\n",
    "        \n",
    "def validate_cnn(model, dev_loader, criterion, optimizer, epoch, DEVICE):\n",
    "    valid_loss = average_meter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "        for i in dev_loader:\n",
    "            x = i[0].to(DEVICE)\n",
    "            y = i[1].to(DEVICE)\n",
    "            l = i[2]\n",
    "            pred = model.forward(x)\n",
    "            loss = criterion(pred, y).to(DEVICE)\n",
    "            valid_loss.update(loss.item(),x.size(0))\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            correct += (pred == y).float().sum()\n",
    "            total += y.shape[0]\n",
    "            step += 1\n",
    "    return valid_loss.avg, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30d75c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Text_CNN, self).__init__()\n",
    "        filter_sizes = [3,5,7,8]\n",
    "        num_filters = 256\n",
    "        n_classes = 7\n",
    "        self.convs1 = nn.ModuleList([nn.Conv1d(1, num_filters, (K, 201)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x) \n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x) \n",
    "        x = self.tanh(x)\n",
    "        output = self.fc3(x) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39759d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing Model\n",
      "setingt optimization method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunch\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0%, Epoch 00, train loss 1.40038791, validate loss 1.30254424, acc 0.5320\n",
      "10.0%, Epoch 01, train loss 0.79001944, validate loss 1.42092471, acc 0.5400\n",
      "15.0%, Epoch 02, train loss 0.37258132, validate loss 1.58403343, acc 0.5600\n",
      "20.0%, Epoch 03, train loss 0.22670102, validate loss 1.71302145, acc 0.5640\n",
      "25.0%, Epoch 04, train loss 0.10596524, validate loss 1.72197108, acc 0.6000\n",
      "30.0%, Epoch 05, train loss 0.05746516, validate loss 1.67223561, acc 0.6160\n",
      "35.0%, Epoch 06, train loss 0.03412827, validate loss 1.87733748, acc 0.6020\n",
      "40.0%, Epoch 07, train loss 0.02110620, validate loss 1.82281574, acc 0.6080\n",
      "45.0%, Epoch 08, train loss 0.01592538, validate loss 1.89111884, acc 0.6100\n",
      "50.0%, Epoch 09, train loss 0.01294396, validate loss 1.91849159, acc 0.6100\n",
      "55.0%, Epoch 10, train loss 0.01207913, validate loss 1.93716675, acc 0.6080\n",
      "60.0%, Epoch 11, train loss 0.01093729, validate loss 1.95263207, acc 0.6180\n",
      "65.0%, Epoch 12, train loss 0.01054615, validate loss 1.94655485, acc 0.6040\n",
      "70.0%, Epoch 13, train loss 0.00986367, validate loss 1.99852439, acc 0.6140\n",
      "75.0%, Epoch 14, train loss 0.00892239, validate loss 1.96157884, acc 0.6100\n",
      "80.0%, Epoch 15, train loss 0.00869342, validate loss 2.02312824, acc 0.6080\n",
      "85.0%, Epoch 16, train loss 0.00798335, validate loss 2.01619485, acc 0.6120\n",
      "90.0%, Epoch 17, train loss 0.00704687, validate loss 2.01782841, acc 0.6060\n",
      "95.0%, Epoch 18, train loss 0.00737156, validate loss 2.01857505, acc 0.6140\n",
      "100.0%, Epoch 19, train loss 0.00675320, validate loss 2.03858316, acc 0.6100\n"
     ]
    }
   ],
   "source": [
    "print('preparing Model')\n",
    "model = Text_CNN().to(DEVICE)\n",
    "print('setingt optimization method')\n",
    "#optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0\n",
    "acc_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = train_cnn(model, train_dataloader, criterion, optimizer, epoch, DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    valid_loss, valid_acc = validate_cnn(model, dev_dataloader, criterion, optimizer, epoch, DEVICE)\n",
    "    if valid_acc > best_acc:\n",
    "        acc_model = deepcopy(model)\n",
    "        best_acc = valid_acc\n",
    "    print(\"{}%, Epoch {}, train loss {:.8f}, validate loss {:.8f}, acc {:.4f}\".format(100*(epoch+1)/n_epochs,str(epoch).zfill(2),train_loss,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf999c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model, train_loader, criterion, optimizer, epoch, DEVICE):\n",
    "    train_loss = average_meter()\n",
    "    model.train()\n",
    "    for x,y,l in train_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        pred = model.forward(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(pred, y).to(DEVICE)\n",
    "        train_loss.update(loss.item(),x.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss.avg\n",
    "        \n",
    "def validate_lstm(model, dev_loader, criterion, optimizer, epoch, DEVICE):\n",
    "    valid_loss = average_meter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "        for x,y,l in dev_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            pred = model.forward(x)\n",
    "            loss = criterion(pred, y).to(DEVICE)\n",
    "            valid_loss.update(loss.item(),x.size(0))\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            correct += (pred == y).float().sum()\n",
    "            total += y.shape[0]\n",
    "            step += 1\n",
    "    return valid_loss.avg, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10020a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Text_LSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=201,hidden_size=128,num_layers=2,\n",
    "                            batch_first=True,bidirectional=True,dropout=0.5)\n",
    "        self.fc1 = nn.Linear(128 * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 7)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = F.relu\n",
    "        self.softmax = F.log_softmax\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input x: [batch_size, seq_len, 301]\n",
    "        After lstm, x:[batch_size, max_len, 2 * hidden_size]\n",
    "        h_n,c_n:[2*num_layers,batch_size,hidden_size]\n",
    "        \"\"\"\n",
    "        x, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # Get the last output in both directions for concat\n",
    "        output_fw = h_n[-2,:,:] # forward's last output\n",
    "        output_bw = h_n[-1,:,:] # backward's last output\n",
    "        output = torch.cat([output_fw,output_bw],dim=-1)\n",
    "\n",
    "        out = self.fc1(output)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out,dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "885ec0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing Model\n",
      "setingt optimization method\n",
      "5.0%, Epoch 00, train loss 1.55199053, validate loss 1.42535348, acc 0.4600\n",
      "10.0%, Epoch 01, train loss 1.19342085, validate loss 1.22011291, acc 0.5620\n",
      "15.0%, Epoch 02, train loss 1.04817154, validate loss 1.20364040, acc 0.5760\n",
      "20.0%, Epoch 03, train loss 0.90413894, validate loss 1.23944250, acc 0.5860\n",
      "25.0%, Epoch 04, train loss 0.79814343, validate loss 1.24439689, acc 0.5740\n",
      "30.0%, Epoch 05, train loss 0.70340649, validate loss 1.27905473, acc 0.5980\n",
      "35.0%, Epoch 06, train loss 0.62085598, validate loss 1.33950342, acc 0.6040\n",
      "40.0%, Epoch 07, train loss 0.52250162, validate loss 1.42534290, acc 0.6160\n",
      "45.0%, Epoch 08, train loss 0.43869680, validate loss 1.60194561, acc 0.5900\n",
      "50.0%, Epoch 09, train loss 0.36563341, validate loss 1.57683130, acc 0.6060\n",
      "55.0%, Epoch 10, train loss 0.31148861, validate loss 1.74726898, acc 0.6100\n",
      "60.0%, Epoch 11, train loss 0.27279553, validate loss 1.71855588, acc 0.6140\n",
      "65.0%, Epoch 12, train loss 0.22239415, validate loss 1.90257329, acc 0.6040\n",
      "70.0%, Epoch 13, train loss 0.19626607, validate loss 2.02800068, acc 0.5920\n",
      "75.0%, Epoch 14, train loss 0.15556146, validate loss 2.16410648, acc 0.5920\n",
      "80.0%, Epoch 15, train loss 0.14149617, validate loss 2.24125620, acc 0.5860\n",
      "85.0%, Epoch 16, train loss 0.11515688, validate loss 2.35936158, acc 0.5900\n",
      "90.0%, Epoch 17, train loss 0.10239076, validate loss 2.37416454, acc 0.5920\n",
      "95.0%, Epoch 18, train loss 0.08697877, validate loss 2.47177546, acc 0.5960\n",
      "100.0%, Epoch 19, train loss 0.07870897, validate loss 2.58073773, acc 0.5960\n"
     ]
    }
   ],
   "source": [
    "print('preparing Model')\n",
    "model = Text_LSTM().to(DEVICE)\n",
    "print('setingt optimization method')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0\n",
    "acc_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = train_lstm(model, train_dataloader, criterion, optimizer, epoch, DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    valid_loss, valid_acc = validate_lstm(model, dev_dataloader, criterion, optimizer, epoch, DEVICE)\n",
    "    if valid_acc > best_acc:\n",
    "        acc_model = deepcopy(model)\n",
    "        best_acc = valid_acc\n",
    "    print(\"{}%, Epoch {}, train loss {:.8f}, validate loss {:.8f}, acc {:.4f}\".format(100*(epoch+1)/n_epochs,str(epoch).zfill(2),train_loss,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbc2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
